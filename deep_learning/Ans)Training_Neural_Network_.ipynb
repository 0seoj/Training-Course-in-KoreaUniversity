{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ans)Training Neural Network_.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "FfWtUiir7XAK",
        "ezH4C21Y8JF4",
        "P7_NK6ue8t6z",
        "nKbhGSix9UQM",
        "xxghrniJ_JIm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HjOvHCGI4FS",
        "colab_type": "text"
      },
      "source": [
        "# Training Neural Network 실습 \n",
        "### Copyright (C) 2018  Cheonbok Park <cb_park@korea.ac.kr>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oOxnJsphGb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn # \n",
        "import torch.nn.functional as F #\n",
        "import torchvision # 이미지 관련 처리, Pretrained Model 관련된 Package 입니다. \n",
        "import torchvision.datasets as vision_dsets\n",
        "import torchvision.transforms as T # 이미지 처리 (Vison) 관련된 transformation이 정의 되어 있습니다.\n",
        "import torch.optim as optim # pytorch 에서 정의한 수 많은 optimization function 들이 들어 있습니다. \n",
        "from torch.utils import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ABEFSDDZ8ON",
        "colab_type": "text"
      },
      "source": [
        "# MNIST Feed-forward Neural Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qije6EXLt9sZ",
        "colab_type": "text"
      },
      "source": [
        "## Data Loader 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo1YYiybty5e",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def MNIST_DATA(root='./',train =True,transforms=None ,download =True,batch_size = 32,num_worker = 1):\n",
        "\n",
        "\tprint (\"[+] Get the MNIST DATA\")\n",
        "\t\"\"\"\n",
        "  \ttorchvision.dataset 에는 우리가 많이 사용하는 데이터들을 쉽게 사용할 수 있도록 되어 있습니다. \n",
        "  \tMachine Learning 에서 Hello world 라고 불리는 Mnist 데이터를 사용해 보겠습니다. \n",
        "  \n",
        "  \n",
        "\t\"\"\"\n",
        "\tmnist_train = vision_dsets.MNIST(root = root,  #root 는 데이터의 저장 위치 입니다. \n",
        "\t\t\t\t\t\t\t\t\ttrain = True, #Train 은 이 데이터가 train 데이터인지 아닌지에 대한 정보입니다. \n",
        "\t\t\t\t\t\t\t\t\ttransform = T.ToTensor(), # 얻어낸 데이터를 pytorch가 계산 할 수 있는 Tensor 로 변환해 줍니다. \n",
        "\t\t\t\t\t\t\t\t\tdownload = True)  # 데이터를 다운로드 할지 여부를 물어봅니다. \n",
        "\tmnist_test = vision_dsets.MNIST(root = root,\n",
        "\t\t\t\t\t\t\t\t\ttrain = False,  # Test Data를 가져오기에 Train =False 를 줘야 합니다. \n",
        "\t\t\t\t\t\t\t\t\ttransform = T.ToTensor(),\n",
        "\t\t\t\t\t\t\t\t\tdownload = True)\n",
        "\t\"\"\"\n",
        "  \tData Loader 는 데이터와 batch size의 정보를 바탕으로 매 iteration 마다 주어진 데이터를 원하는 batch size 만큼 반환해주는 iterator입니다. \n",
        "  \t* Practical Guide : Batch size 는 어느정도가 좋나요? -- 클 수록 좋다는 소리가 있습니다. 하지만 gpu memeory 사이즈 한계에 의해 기본적으로 batch size 가 \n",
        "  \t커질 수록 학습에 사용되는 gpu memory 사이즈가 큽니다. (Activation map을 저장해야 하기 때문입니다.) 기본적으로 2의 배수로 저장하는 것이 좋습니다.(Bit size 관련) \n",
        "  \n",
        "\t\"\"\"\n",
        "\ttrainDataLoader = data.DataLoader(dataset = mnist_train,  # DataSet은 어떤 Data를 제공해 줄지에 대한 정보입니다. 여기서는 Training DATA를 제공합니다. \n",
        "\t\t\t\t\t\t\t\t\tbatch_size = batch_size, # batch size 정보를 꼭 줘야 합니다. 한 Batch 당 몇 개의 Data 를 제공할지에 대한 정보입니다. \n",
        "\t\t\t\t\t\t\t\t\tshuffle =True, # Training의 경우 Shuffling 을 해주는 것이 성능에 지대한 영향을 끼칩니다. 꼭 True 를 줘야 합니다. \n",
        "\t\t\t\t\t\t\t\t\tnum_workers = 1) # num worker의 경우 데이터를 로드하는데 worker를 얼마나 추가하겠는가에 대한 정보입니다. \n",
        "\n",
        "\ttestDataLoader = data.DataLoader(dataset = mnist_test, # Test Data Loader 이므로 Test Data를 인자로 전달해줍니다.\n",
        "\t\t\t\t\t\t\t\t\tbatch_size = batch_size, # 마찬가지로 Batch size 를 넣어줍니다. \n",
        "\t\t\t\t\t\t\t\t\tshuffle = False, # shuffling 이 굳이 필요하지 않으므로 false를 줍니다. \n",
        "\t\t\t\t\t\t\t\t\tnum_workers = 1) #\n",
        "\tprint (\"[+] Finished loading data & Preprocessing\")\n",
        "\treturn mnist_train,mnist_test,trainDataLoader,testDataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P2oJog_xnFJ",
        "colab_type": "code",
        "outputId": "484ef883-1721-48dd-efec-7ebafbc6ad5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "trainDset,testDset,trainDataLoader,testDataLoader= MNIST_DATA(batch_size = 32)  # Data Loader 를 불러 옵니다. "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[+] Get the MNIST DATA\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:07, 1311695.93it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 57057.85it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 863763.19it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21664.67it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "[+] Finished loading data & Preprocessing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtWA2FTIxskN",
        "colab_type": "text"
      },
      "source": [
        "## Train Function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA8glX3ox0Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(net,optimizer,trainloader):\n",
        "  for epoch in range(4):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0 # running loss를 저장하기 위한 변수입니다. \n",
        "      for i, data in enumerate(trainloader, 0): # 한 Epoch 만큼 돕니다. 매 iteration 마다 정해진 Batch size 만큼 데이터를 뱉습니다. \n",
        "          # get the inputs\n",
        "          inputs, labels = data # DataLoader iterator의 반환 값은 input_data 와 labels의 튜플 형식입니다. \n",
        "          inputs = inputs.cuda() # gpu에 데이터를 올립니다.\n",
        "          labels = labels.cuda()\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()    #  현재 기존의 backprop을 계산하기 위해서 저장했던 activation buffer 를 비웁니다. Q) 이걸 안 한다면?\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs) # input 을 넣은 위 network 로 부터 output 을 얻어냅니다. \n",
        "          loss = criterion(outputs, labels) # loss fucntion에 주어진 target과 output 의 score를 계산하여 반환합니다. \n",
        "          loss.backward() # * Scalar Loss value를 Backward() 해주게 되면 주어진 loss값을 바탕으로 backpropagation이 진행됩니다. \n",
        "          optimizer.step() # 계산된 Backprop 을 바탕으로 optimizer가 gradient descenting 을 수행합니다. \n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 500 == 499:    # print every 2000 mini-batches\n",
        "              print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 500))\n",
        "              running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGMkGKQiy8_r",
        "colab_type": "text"
      },
      "source": [
        "## Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Xn-H2zbc22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model,test_loader):\n",
        "  model.eval() # Eval Mode 왜 해야 할까요?  --> nn.Dropout BatchNorm 등의 Regularization 들이 test 모드로 들어가게 되기 때문입니다. \n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data, target = data.cuda(), target.cuda()  # 기존의 train function의 data 처리부분과 같습니다. \n",
        "    output = model(data) \n",
        "    pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
        "    correct += pred.eq(target.view_as(pred)).sum().item() # 정답 데이터의 갯수를 반환합니다. \n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      correct, len(test_loader.dataset),\n",
        "      100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdM-bsXD152f",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network  + Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmv3W0Ln29M1",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (1)\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Sigmoid \n",
        "\n",
        "Layer 2 - input: 30 output:10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTJOsz9U2FnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__() # nn.Module 생성자 호출 Q) 왜 필요할까요? A) nn.module의 경우 자동적으로 backpropagation compute를 해주는 함수를 만들어주게 해줌.\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30)\n",
        "        self.fc1 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,28*28) # x.view함수는 주어진 인자의 크기로 해당 데이터의 크기를 반환합니다. 즉, (Batch_size,28,28) --> (Batch_size,28*28)로 변환합니다.\n",
        "        x = F.sigmoid(self.fc0(x)) # 28*28 -> 30 -> Activation function 을 수행합니다.\n",
        "        x = self.fc1(x)  # 30 -> 10 으로 10개의 Class에 대한 logit 값을 호출합니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvk4TTL83pcc",
        "colab_type": "text"
      },
      "source": [
        "#### Optimizer \n",
        "Optimizer 의 경우 기본적으로 torch.optim 안에 존재합니다. 다양한 optimziers 가 정의되어 있습니다. \n",
        "\n",
        "기본적으로 다음과 같은 구성을 따릅니다. optim.{Optimzier 이름}({Network Parameters},lr ={learning rate })"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd5tmyMnXcs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zALyUje-aehn",
        "colab_type": "code",
        "outputId": "28bd0d2e-5bbe-4435-b293-d6826e77b16e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader) # 4 Epoch 정도 학습을 진행해봅니다. "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.315\n",
            "[1,  1000] loss: 2.290\n",
            "[1,  1500] loss: 2.276\n",
            "[2,   500] loss: 2.253\n",
            "[2,  1000] loss: 2.240\n",
            "[2,  1500] loss: 2.226\n",
            "[3,   500] loss: 2.204\n",
            "[3,  1000] loss: 2.190\n",
            "[3,  1500] loss: 2.175\n",
            "[4,   500] loss: 2.149\n",
            "[4,  1000] loss: 2.130\n",
            "[4,  1500] loss: 2.113\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfQrMIX2a3nn",
        "colab_type": "code",
        "outputId": "46b0a193-2931-47b2-94ae-3f70d959cebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "test(mnist_net,testDataLoader) # Test 정확도를 출력해 봅니다. "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 5576/10000 (56%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUK2GbHZ4x-K",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (2)\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - tanh \n",
        "\n",
        "Layer 2 - input: 30 output:10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZUmc76ZdOap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__() # nn.Module 생성자 호출 Q) 왜 필요할까요?\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30)\n",
        "        self.fc1 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,28*28) # x.view함수는 주어진 인자의 크기로 해당 데이터의 크기를 반환합니다. 즉, (Batch_size,28,28) --> (Batch_size,28*28)로 변환합니다.\n",
        "        x = F.tanh(self.fc0(x)) # 28*28 -> 30 -> Activation function 을 수행합니다.\n",
        "        x = self.fc1(x)  # 30 -> 10 으로 10개의 Class에 대한 logit 값을 호출합니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBXketXnhA_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb3e4wqphDmZ",
        "colab_type": "code",
        "outputId": "7dfd1b43-209a-45e5-d4c3-5da70e2e7552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.227\n",
            "[1,  1000] loss: 2.070\n",
            "[1,  1500] loss: 1.923\n",
            "[2,   500] loss: 1.671\n",
            "[2,  1000] loss: 1.538\n",
            "[2,  1500] loss: 1.422\n",
            "[3,   500] loss: 1.248\n",
            "[3,  1000] loss: 1.165\n",
            "[3,  1500] loss: 1.104\n",
            "[4,   500] loss: 0.990\n",
            "[4,  1000] loss: 0.944\n",
            "[4,  1500] loss: 0.905\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ-WCpKrhEzR",
        "colab_type": "code",
        "outputId": "0c5e09b9-e5c0-4437-e9d8-54c7213ed276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 8247/10000 (82%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_aGry05TIV",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (3)\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu\n",
        "\n",
        "Layer 2 - input: 30 output:10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvAhsyP-hbwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30)\n",
        "        self.fc1 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x))\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZVpqqCbhiNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKGyPdSIhmqx",
        "colab_type": "code",
        "outputId": "e0e04905-6091-4e63-c0f8-91b8961461cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.264\n",
            "[1,  1000] loss: 2.159\n",
            "[1,  1500] loss: 2.037\n",
            "[2,   500] loss: 1.774\n",
            "[2,  1000] loss: 1.609\n",
            "[2,  1500] loss: 1.453\n",
            "[3,   500] loss: 1.205\n",
            "[3,  1000] loss: 1.108\n",
            "[3,  1500] loss: 1.011\n",
            "[4,   500] loss: 0.885\n",
            "[4,  1000] loss: 0.823\n",
            "[4,  1500] loss: 0.783\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_tEaqWlhnUQ",
        "colab_type": "code",
        "outputId": "5c32edea-85e6-448d-eea3-ae46f924612c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 8454/10000 (85%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfWtUiir7XAK",
        "colab_type": "text"
      },
      "source": [
        "### Q) 성능차이가 존재하나요? 존재한다면 무슨 이유일까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abXo7TN-3zHH",
        "colab_type": "text"
      },
      "source": [
        "실제로 돌려보면 (1)이 가장 낮고 (2) (3) 순으로 성능이 좋습니다. 그 이유는 sigmoid의 경우 최대 gradient 값이 0에서 0.25이고 gradient vanishing 이 일어나기 쉬우나, tanh는 상대적으로 gradient 값이 0에서 sigmoid보다 2배 더 크기때문에 좀 더 학습이 잘 되기 때문에 성능이 좋습니다. relu의 경우 sigmoid,tanh와 달리  gradient vanshing 문제가 없기 때문에 적은 epoch만에 좋은 성능에도달하게 됩니다. . "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQALVsvI5ful",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (4) \n",
        "특징 : 3개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - sigmoid \n",
        "\n",
        "Layer 2 - input: 40 output: 30\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLEQ6RZXjOCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        self.fc0 = nn.Linear(28*28,40) # Layer 1\n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.sigmoid(self.fc0(x))\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MM_WIY6jVG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kWON9jtjXAy",
        "colab_type": "code",
        "outputId": "9136a304-7f94-4bae-f6b5-a48d948ef841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.336\n",
            "[1,  1000] loss: 2.317\n",
            "[1,  1500] loss: 2.309\n",
            "[2,   500] loss: 2.302\n",
            "[2,  1000] loss: 2.301\n",
            "[2,  1500] loss: 2.300\n",
            "[3,   500] loss: 2.300\n",
            "[3,  1000] loss: 2.299\n",
            "[3,  1500] loss: 2.299\n",
            "[4,   500] loss: 2.299\n",
            "[4,  1000] loss: 2.298\n",
            "[4,  1500] loss: 2.299\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaCuXaHWjXzH",
        "colab_type": "code",
        "outputId": "66dfd03c-08e2-459d-88d3-ef26400549fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 1135/10000 (11%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxqP1tY965JF",
        "colab_type": "text"
      },
      "source": [
        "### Q) 학습이 잘 되나요???? 안 된다면 왜 안될까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB-xPEgZ5Z7m",
        "colab_type": "text"
      },
      "source": [
        "오히려 (1) 보다 (4)가 gradient vanshing이 더 심하게 생겼기 때문입니다. 쉽게 생각해보면 sigmoid의 최대 기울기는 0.25이기때문에 레이어를 거침에 따라 이 값은 더 작아질 수 밖에 없습니다. 그래서 첫 번째 레이어의 경우에는 오히려 최대 기울기가 0.16밖에 되지 않습니다. 그러다보니 vanishing gradient이 자주 일어나  훨씬 학습이 느리고 잘 되지 않습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZIkQe9f6tdy",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (5) \n",
        "특징 : 3개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu \n",
        "\n",
        "Layer 2 - input: 40 output: 30\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uHrf8bRjZFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x)) # Layer 1\n",
        "        x = F.relu(self.fc1(x)) # Layer 2\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEtXES71j9s6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxHiSRSskasK",
        "colab_type": "code",
        "outputId": "83f07f98-2b1a-49be-dfab-2e829611428f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.304\n",
            "[1,  1000] loss: 2.294\n",
            "[1,  1500] loss: 2.281\n",
            "[2,   500] loss: 2.252\n",
            "[2,  1000] loss: 2.230\n",
            "[2,  1500] loss: 2.200\n",
            "[3,   500] loss: 2.130\n",
            "[3,  1000] loss: 2.073\n",
            "[3,  1500] loss: 2.001\n",
            "[4,   500] loss: 1.836\n",
            "[4,  1000] loss: 1.718\n",
            "[4,  1500] loss: 1.591\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0woJ7sbzkbqj",
        "colab_type": "code",
        "outputId": "520d6d4b-1b80-4223-c0b5-811e4d8e182f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 6672/10000 (67%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YCjGFh77xeq",
        "colab_type": "text"
      },
      "source": [
        "#### (4)와 차이가 존재하나요? 그렇다면 왜 그럴까요? (3) 이랑은 비교해보면 어떻나요? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiFgFB0JF6G8",
        "colab_type": "text"
      },
      "source": [
        "상대적으로 (3)과 비교해보면 vanishing gradient 가 덜 생기게 되어 정확도는 높은 걸 확인 할 수 있습니다. 하지만, (3)에 비해서는 성능이 낮은데요, 레이어가 깊어짐에 따라 backpropagation수식을 생각해 보면 WT(w transpose)부분이 곱해져 gradeint 값이 작아질수 밖에 없는 것을 확인할 수 있죠. relu여도 이런 gradient vanshing 이 일어날수가 있어서 추후에 resnet의 skipconnection 과 batchnorm같은걸 사용해서 학습을 빠르게 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezH4C21Y8JF4",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (6) \n",
        "특징 : 3개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu \n",
        "\n",
        "Layer 2 - input: 40 output: 30\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-oDSJBL8Wk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x)) # Layer 1\n",
        "        x = F.relu(self.fc1(x)) # Layer 2\n",
        "        x = self.fc2(x) # Layer 3 \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJEAkxqX8YWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVGBPs3W8dRd",
        "colab_type": "code",
        "outputId": "089193da-dbd1-4957-8ce4-15a3be1a1c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.702\n",
            "[1,  1000] loss: 0.322\n",
            "[1,  1500] loss: 0.276\n",
            "[2,   500] loss: 0.217\n",
            "[2,  1000] loss: 0.192\n",
            "[2,  1500] loss: 0.191\n",
            "[3,   500] loss: 0.160\n",
            "[3,  1000] loss: 0.154\n",
            "[3,  1500] loss: 0.145\n",
            "[4,   500] loss: 0.123\n",
            "[4,  1000] loss: 0.126\n",
            "[4,  1500] loss: 0.123\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKcDCwPJ8dqP",
        "colab_type": "code",
        "outputId": "808e3b59-1787-4bc9-eb83-7366d4b48e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9604/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7_NK6ue8t6z",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (7) Layer 를 줄여볼까요? \n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu \n",
        "\n",
        "Layer 2 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjnnxkiG8slU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30) #Layer 1 \n",
        "        self.fc1 =  nn.Linear(30, 10) # Layer 2\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x)) # Layer 1\n",
        "        x = self.fc1(x) # Layer 2\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQea1DZS8s2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wPfgH1S8tFP",
        "colab_type": "code",
        "outputId": "8ab31da8-77e1-43a4-f6ab-23280d4bfde8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.669\n",
            "[1,  1000] loss: 0.329\n",
            "[1,  1500] loss: 0.293\n",
            "[2,   500] loss: 0.241\n",
            "[2,  1000] loss: 0.223\n",
            "[2,  1500] loss: 0.209\n",
            "[3,   500] loss: 0.173\n",
            "[3,  1000] loss: 0.182\n",
            "[3,  1500] loss: 0.161\n",
            "[4,   500] loss: 0.144\n",
            "[4,  1000] loss: 0.142\n",
            "[4,  1500] loss: 0.137\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8vW8d-18-Ix",
        "colab_type": "code",
        "outputId": "a506c72b-9224-46f3-efc8-879ab1b75b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9587/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKbhGSix9UQM",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (7) Batch Norm 을 줘 볼까요?\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu  + Batch Norm\n",
        "\n",
        "Layer 2 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpakqWPv9cs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30) #Layer 1\n",
        "        self.bn0 = nn.BatchNorm1d(30) # BatchNorm \n",
        "        self.fc1 =  nn.Linear(30, 10) # Layer 2\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.bn0(self.fc0(x))) # Layer 1\n",
        "        x = self.fc1(x) # Layer 2\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suyt84BXHzE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHANHwQT9c6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el8EvYky9dgz",
        "colab_type": "code",
        "outputId": "296307dd-2dce-4008-b6fa-266857dbb2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.896\n",
            "[1,  1000] loss: 0.366\n",
            "[1,  1500] loss: 0.293\n",
            "[2,   500] loss: 0.234\n",
            "[2,  1000] loss: 0.217\n",
            "[2,  1500] loss: 0.200\n",
            "[3,   500] loss: 0.172\n",
            "[3,  1000] loss: 0.183\n",
            "[3,  1500] loss: 0.176\n",
            "[4,   500] loss: 0.150\n",
            "[4,  1000] loss: 0.157\n",
            "[4,  1500] loss: 0.150\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIUIeYYe9rid",
        "colab_type": "code",
        "outputId": "6d5f94e9-7a20-4a33-f6d5-69baa02f6c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9620/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBTuBB9_93mq",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (8) 더 깊은 레이어에 Batch Norm 을 줘 볼까요?\n",
        "특징 : 2개의 Layer를 가지는 Neural Network\n",
        "\n",
        "<구성>  \n",
        "\n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu + BatchNorm\n",
        "\n",
        "Layer 2 - input: 40 output: 30 + Activation Fucntion - Relu  + BatchNorm\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgGaaFW8-Ny7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
        "        self.bn0 = nn.BatchNorm1d(40) #BatchNorm1 \n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.bn1 = nn.BatchNorm1d(30) #BatchNorm1 \n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.bn0(self.fc0(x))) # Layer 1\n",
        "        x = F.relu(self.bn1(self.fc1(x))) # Layer 2\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClLHQwZE-OCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxHuM-IF-nz2",
        "colab_type": "code",
        "outputId": "2337bee3-ddf3-46de-a931-685d518b2b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.845\n",
            "[1,  1000] loss: 0.282\n",
            "[1,  1500] loss: 0.220\n",
            "[2,   500] loss: 0.164\n",
            "[2,  1000] loss: 0.159\n",
            "[2,  1500] loss: 0.155\n",
            "[3,   500] loss: 0.135\n",
            "[3,  1000] loss: 0.128\n",
            "[3,  1500] loss: 0.121\n",
            "[4,   500] loss: 0.103\n",
            "[4,  1000] loss: 0.115\n",
            "[4,  1500] loss: 0.109\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK1kspbG-qB9",
        "colab_type": "code",
        "outputId": "f3ca70b4-dc8f-4b28-aac6-a45c36ee1fce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9707/10000 (97%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxghrniJ_JIm",
        "colab_type": "text"
      },
      "source": [
        "#### Batch Normalization 을 적용한 (7)과 (6)을 비교해보고 (8) 과 (5)를 비교해보면 어떻나요? 학습이 어떻게 달라졌을까요? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwSFl9v_c4F",
        "colab_type": "text"
      },
      "source": [
        "### Let's Do it - 성능을 한번 끝까지 높여볼까요~? 마음대로 한번 최고 성능을 찍어봅시다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjM6p0gvkccx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        \n",
        "        self.fc0 = nn.Linear(28*28,16*5*5)\n",
        "        self.bn0 = nn.BatchNorm1d(16*5*5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 84)\n",
        "        self.bn1 = nn.BatchNorm1d(84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.bn0(self.fc0(x)))\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGsCsD6blqBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wPzE5N0lq-Y",
        "colab_type": "code",
        "outputId": "98837cb0-c649-4632-aa48-60cb54de0e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.469\n",
            "[1,  1000] loss: 0.182\n",
            "[1,  1500] loss: 0.149\n",
            "[2,   500] loss: 0.105\n",
            "[2,  1000] loss: 0.100\n",
            "[2,  1500] loss: 0.096\n",
            "[3,   500] loss: 0.062\n",
            "[3,  1000] loss: 0.070\n",
            "[3,  1500] loss: 0.071\n",
            "[4,   500] loss: 0.049\n",
            "[4,  1000] loss: 0.056\n",
            "[4,  1500] loss: 0.056\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhuKrwYElsAj",
        "colab_type": "code",
        "outputId": "7f4b393e-c283-47f5-9f07-a07a5fc7cdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9791/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxip6TQc_txE",
        "colab_type": "text"
      },
      "source": [
        "## Practical Guide Pytorch nn.Sequential "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDexfQHd_72G",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "x = F.relu(self.bn0(self.fc0(x)))\n",
        "x = F.relu(self.bn1(self.fc1(x)))\n",
        "```\n",
        "너무 복잡하지 않나요?  그냥 x = self.fc(x) 쉽게 해버리면 안 될까요?\n",
        "\n",
        "Solution : nn.Sequential + 자매품 nn.ModuList\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59l4rpz5ls39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        \n",
        "        layer_list = [] # 이 리스트에 모든 Layer 를 순차적으로 append 해보겠습니다. \n",
        "        layer_list.append(nn.Linear(28*28,40)) #Layer 1 \n",
        "        layer_list.append(nn.ReLU())\n",
        "        layer_list.append(nn.BatchNorm1d(40))#BatchNorm1 \n",
        "        layer_list.append(nn.Linear(40, 30)) # Layer 2\n",
        "        layer_list.append(nn.ReLU())\n",
        "        layer_list.append(nn.BatchNorm1d(30)) #BatchNorm1 \n",
        "        layer_list.append(nn.Linear(30, 10)) # Layer 3\n",
        "        self.net  = nn.Sequential(*layer_list) # nn.Sequential 에 layer list를 넘겨 줍니다.\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = self.net(x) # 넣은 순서대로 적용이 됩니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRRIS8y1mOOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF5Tr8dXBMmg",
        "colab_type": "code",
        "outputId": "4ce8705e-4989-4ac7-a420-ac64ebf5da3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.844\n",
            "[1,  1000] loss: 0.338\n",
            "[1,  1500] loss: 0.290\n",
            "[2,   500] loss: 0.227\n",
            "[2,  1000] loss: 0.215\n",
            "[2,  1500] loss: 0.193\n",
            "[3,   500] loss: 0.162\n",
            "[3,  1000] loss: 0.159\n",
            "[3,  1500] loss: 0.151\n",
            "[4,   500] loss: 0.130\n",
            "[4,  1000] loss: 0.121\n",
            "[4,  1500] loss: 0.118\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7o8XOAlBSFB",
        "colab_type": "code",
        "outputId": "61d3a817-179f-4c16-d4bb-8a87b0a8d34d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9626/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysv7uX-1Dh0b",
        "colab_type": "text"
      },
      "source": [
        "#### 연습해 봅시다 ! \n",
        "\n",
        "특징 : 2개의 Layer를 가지는 Neural Network <구성>\n",
        "\n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu + Batch Norm\n",
        "\n",
        "Layer 2 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss + Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-esUcJ8VDu_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        \n",
        "        layer_list = [] # 이 리스트에 모든 Layer 를 순차적으로 append 해보겠습니다. \n",
        "        layer_list.append(nn.Linear(28*28,30)) #Layer 1 \n",
        "        layer_list.append(nn.ReLU())\n",
        "        layer_list.append(nn.BatchNorm1d(30)) #BatchNorm1 \n",
        "        layer_list.append(nn.Linear(30, 10)) # Layer 2\n",
        "        self.net  = nn.Sequential(*layer_list) # nn.Sequential 에 layer list를 넘겨 줍니다.\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = self.net(x) # 넣은 순서대로 적용이 됩니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaktO6PvD0M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9She_4cMD1ES",
        "colab_type": "code",
        "outputId": "0146b8cf-77b0-491f-c5f1-68b11a03f6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.760\n",
            "[1,  1000] loss: 0.383\n",
            "[1,  1500] loss: 0.352\n",
            "[2,   500] loss: 0.330\n",
            "[2,  1000] loss: 0.329\n",
            "[2,  1500] loss: 0.324\n",
            "[3,   500] loss: 0.305\n",
            "[3,  1000] loss: 0.308\n",
            "[3,  1500] loss: 0.323\n",
            "[4,   500] loss: 0.301\n",
            "[4,  1000] loss: 0.302\n",
            "[4,  1500] loss: 0.313\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kseseeW4D1zc",
        "colab_type": "code",
        "outputId": "ccccb43b-17f0-4da2-aac3-e463a65901b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9204/10000 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}